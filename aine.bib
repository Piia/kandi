@inproceedings{RefWorks:doc:59c154d7e4b00694fa917fd2,
	author={Sebastian Elbaum and Gregg Rothermel and John Penix},
	year={2014},
	title={Techniques for Improving Regression Testing in Continuous Integration Development Environments},
	publisher={ACM},
	address={New York, NY, USA},
	pages={235$\-$245},
	abstract={In continuous integration development environments, software engineers frequently integrate new or changed code with the mainline codebase. This can reduce the amount of code rework that is needed as systems evolve and speed up development time. While continuous integration processes traditionally require that extensive testing be performed following the actual submission of code to the codebase, it is also important to ensure that enough testing is performed prior to code submission to avoid breaking builds and delaying the fast feedback that makes continuous integration desirable. In this work, we present algorithms that make continuous integration processes more cost$\-$effective. In an initial pre$\-$submit phase of testing, developers specify modules to be tested, and we use regression test selection techniques to select a subset of the test suites for those modules that render that phase more cost$\-$effective. In a subsequent post$\-$submit phase of testing, where dependent modules as well as changed modules are tested, we use test case prioritization techniques to ensure that failures are reported more quickly. In both cases, the techniques we utilize are novel, involving algorithms that are relatively inexpensive and do not rely on code coverage information $\-$$\-$ two requirements for conducting testing cost$\-$effectively in this context. To evaluate our approach, we conducted an empirical study on a large data set from Google that we make publicly available. The results of our study show that our selection and prioritization techniques can each lead to cost$\-$effectiveness improvements in the continuous integration process.},
	url={http://doi.acm.org/10.1145/2635868.2635910},
	doi={10.1145/2635868.2635910}
}
@inproceedings{RefWorks:doc:59c154bee4b083d4aaa16b45,
	author={Atif Memon and Zebao Gao and Bao Nguyen and Sanjeev Dhanda and Eric Nickell and Rob Siemborski and John Micco},
	year={2017},
	title={Taming Google$\-$scale Continuous Testing},
	publisher={IEEE Press},
	address={Piscataway, NJ, USA},
	pages={233$\-$242},
	abstract={Growth in Google's code size and feature churn rate has seen increased reliance on continuous integration (CI) and testing to maintain quality. Even with enormous resources dedicated to testing, we are unable to regression test each code change individually, resulting in increased lag time between code check$\-$ins and test result feedback to developers. We report results of a project that aims to reduce this time by: (1) controlling test workload without compromising quality, and (2) distilling test results data to inform developers, while they write code, of the impact of their latest changes on quality. We model, empirically understand, and leverage the correlations that exist between our code, test cases, developers, programming languages, and code$\-$change and test$\-$execution frequencies, to improve our CI and development processes. Our findings show: very few of our tests ever fail, but those that do are generally "closer" to the code they test; certain frequently modified code and certain users/tools cause more breakages; and code recently modified by multiple developers (more than 3) breaks more often.},
	url={https://doi.org/10.1109/ICSE$\-$SEIP.2017.16},
	doi={10.1109/ICSE$\-$SEIP.2017.16}
}
@inproceedings{RefWorks:doc:59c154b0e4b029007aae2a5a,
	author={de S. Campos Junior, Heleno and Camila Ac√°cio de Paiva and Regina Braga and Marco Ant√¥nio P. Ara√∫jo and Jos√© Maria N. David and Fernanda Campos},
	year={2017},
	title={Regression Tests Provenance Data in the Continuous Software Engineering Context},
	publisher={ACM},
	address={New York, NY, USA},
	pages={10:1$\-$10:6},
	abstract={Regression tests are executed after every change in software. In a software development environment that adopts Continuous Software Engineering practices such as Continuous Integration, Continuous Delivery and Continuous Deployment, software is changed, built and tested many times. Every regression test execution may include different situations and problems that are treated in isolated way. Data provenance is concerned with the origins and processes that some data has gone through, until it becomes information. Ontologies are formal models that contain axioms and relationships between classes and individuals from a specific context and can be used to infer implicit knowledge. Considering that Continuous Software Engineering activities are based on feedback cycles, in this paper, we propose an architecture based on the use of an ontology and provenance model to capture and provide regression tests data to support the continuous improvement of software testing processes. Moreover, using ontology and provenance to track execution performance and issues in this scenario, may increase the chances of those issues not happening again, since practitioners can address and solve them for future executions.},
	url={http://doi.acm.org/10.1145/3128473.3128483},
	doi={10.1145/3128473.3128483}
}
@inproceedings{RefWorks:doc:59c1195ce4b063d049a381fc,
	author={Sebastian Vˆst and Stefan Wagner},
	year={2016},
	title={Trace$\-$based Test Selection to Support Continuous Integration in the Automotive Industry},
	publisher={ACM},
	address={New York, NY, USA},
	pages={34$\-$40},
	abstract={System testing in the automotive industry is a very expensive and time$\-$consuming task of growing importance, because embedded systems in the domain are distributed over numerous controllers (ECUs). Modern software development techniques such as continuous integration require regular, repeated and fast testing. To achieve this in the automotive domain, test suites for a specific software change must be tailored. We propose a novel test selection technique for system$\-$level functions in the automotive industry based on component and communication models. The idea is to follow input and output signals that are used in the testing steps through the ECUs implementing a function. We select only those tests for a planned integration in which at least one of the signals sent in its steps is processed by the ECU that was changed and thus triggered the integration. The technique is well$\-$suited for black$\-$box testing since it requires only the full test suite specification and the system architecture. We applied the technique to a test suite of the Active Cruise Control function at BMW Group in the context of hardware$\-$in$\-$the$\-$loop system testing and found the possible reduction rates to be 82.3$\$\%$$ on average in comparison to the full test suite. Possible future work includes the evaluation with a wider set of functions, the evaluation of the fault detection rate, further automation and combination with other test selection techniques.},
	url={http://doi.acm.org/10.1145/2896941.2896951},
	doi={10.1145/2896941.2896951}
}
@inproceedings{RefWorks:doc:59c1194fe4b00694fa915ba1,
	author={Adriaan Labuschagne and Laura Inozemtseva and Reid Holmes},
	year={2017},
	title={Measuring the Cost of Regression Testing in Practice: A Study of Java Projects Using Continuous Integration},
	publisher={ACM},
	address={New York, NY, USA},
	pages={821$\-$830},
	abstract={Software defects cost time and money to diagnose and fix. Consequently, developers use a variety of techniques to avoid introducing defects into their systems. However, these techniques have costs of their own; the benefit of using a technique must outweigh the cost of applying it. In this paper we investigate the costs and benefits of automated regression testing in practice. Specifically, we studied 61 projects that use Travis CI, a cloud$\-$based continuous integration tool, in order to examine real test failures that were encountered by the developers of those projects. We determined how the developers resolved the failures they encountered and used this information to classify the failures as being caused by a flaky test, by a bug in the system under test, or by a broken or obsolete test. We consider that test failures caused by bugs represent a benefit of the test suite, while failures caused by broken or obsolete tests represent a test suite maintenance cost. We found that 18$\%$ of test suite executions fail and that 13$\$\%$$ of these failures are flaky. Of the non$\-$flaky failures, only 74$\$\%$$ were caused by a bug in the system under test; the remaining 26$\%$ were due to incorrect or obsolete tests. In addition, we found that, in the failed builds, only 0.38$\%$ of the test case executions failed and 64$\%$ of failed builds contained more than one failed test. Our findings contribute to a wider understanding of the unforeseen costs that can impact the overall cost effectiveness of regression testing in practice. They can also inform research into test case selection techniques, as we have provided an approximate empirical bound on the practical value that could be extracted from such techniques. This value appears to be large, as the 61 systems under study contained nearly 3 million lines of test code and yet over 99$\%$ of test case executions could have been eliminated with a perfect oracle.},
	url={http://doi.acm.org/10.1145/3106237.3106288},
	doi={10.1145/3106237.3106288}
}
